"""
Router module for deciding how to answer user queries
"""
import re
import logging
import os
from langchain_community.chat_models import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def choose_route(question, ragie_chunks):
    """
    Determine the appropriate route for answering a question
    using LLM-based routing with pattern matching fallback
    
    Args:
        question: User question
        ragie_chunks: List of chunks retrieved from Ragie
        
    Returns:
        str: Route choice - "ragie", "force_ragie", "web", or "base"
    """
    # Normalize question for analysis
    q_lower = question.lower().strip()
    
    # Use LLM to determine if this is a web search query
    if is_web_search_query(question):
        logger.info("Routing to 'web' based on LLM decision")
        return "web"
    
    # For non-web-search queries, use existing logic
    if ragie_chunks:
        logger.info("Routing to 'ragie' because chunks were found")
        return "ragie"
    
    # Check for base LLM indicators (questions that don't need external knowledge)
    base_llm_patterns = [
        # General advice or opinion questions
        r'^(what (do|would) you|how (do|would|can) you)',
        r'^(can you|could you) (help|advise|suggest)',
        
        # Hypothetical scenarios
        r'(if|imagine|suppose|what if)',
        
        # Personal coaching questions
        r'(how (can|should) i|what (can|should) i)',
        r'(advice|suggestion|tip|help) (for|with|on)',
        
        # Simple factual questions that a general LLM should know
        r'^(what is|who is|how does) (a|an|the)',
    ]
    
    # Check if any base LLM patterns match
    for pattern in base_llm_patterns:
        if re.search(pattern, q_lower):
            logger.info(f"Routing to 'base' because pattern matched: {pattern}")
            return "base"
    
    # Default to trying RAG again with a modified query
    # This implements the advanced RAG optimization approach
    logger.info(f"Routing to 'force_ragie' as default")
    return "force_ragie"

def is_web_search_query(question):
    """
    Use the LLM to determine if a query requires web search
    
    Args:
        question: User question
        
    Returns:
        bool: True if the query should use web search
    """
    # First, try a quick check for obvious weather queries to avoid LLM call
    if is_obvious_weather_query(question.lower()):
        logger.info("Detected obvious weather query, skipping LLM call")
        return True
    
    # Check if OpenAI API key is available
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        logger.error("OPENAI_API_KEY environment variable not set")
        # Fall back to pattern matching
        return check_time_sensitive_patterns(question.lower())
    
    # Create the chat model (using a smaller/cheaper model is fine for this task)
    try:
        model = ChatOpenAI(
            temperature=0.1,  # Low temperature for consistent results
            model_name="gpt-3.5-turbo",  # Could use a smaller model to save costs
            openai_api_key=openai_api_key,
            max_tokens=50  # We only need a short response
        )
        
        # Create the prompt
        system_content = """
        You are a routing assistant for an AI Coach application. Your job is to determine if a user query requires web search.
        
        Respond ONLY with "Web Search" or "Not Web Search".
        
        Use "Web Search" for:
        - Questions about current events, news, or recent developments
        - Weather forecasts or current conditions
        - Sports scores or recent game results
        - Stock prices or market information
        - Questions containing time indicators like today, yesterday, tomorrow
        - Questions about what's currently happening or trending
        - Questions about protests, events, or incidents that happened recently
        
        Use "Not Web Search" for:
        - General advice or coaching questions
        - Questions about concepts, theories, or timeless information
        - Personal development or self-improvement questions
        - Questions that don't require up-to-date information
        """
        
        # Create messages
        messages = [
            SystemMessage(content=system_content),
            HumanMessage(content=f"Query: {question}")
        ]
        
        # Get the response
        logger.info(f"Sending query to LLM for routing decision: {question}")
        response = model(messages)
        result = response.content.strip()
        
        logger.info(f"LLM routing decision for '{question}': {result}")
        
        # Return True if the response contains "Web Search"
        return "web search" in result.lower()
        
    except Exception as e:
        logger.error(f"Error in LLM routing: {str(e)}")
        # Fall back to pattern matching if LLM fails
        logger.info("Falling back to pattern matching for routing decision")
        return check_time_sensitive_patterns(question.lower())

def is_obvious_weather_query(q_lower):
    """
    Quick check for obvious weather queries to avoid LLM call
    
    Args:
        q_lower: Lowercase question
        
    Returns:
        bool: True if it's obviously a weather query
    """
    weather_terms = ["weather", "forecast", "temperature", "rain", "snow", "sunny", "cloudy"]
    time_terms = ["today", "tomorrow", "tonight", "this week", "this weekend"]
    
    # Check for common weather question patterns
    weather_patterns = [
        r"what('s| is) the weather (like |going to be |forecast |)",
        r"(will|is) it (rain|snow|be sunny|be cloudy)",
        r"(how|what)('s| is) the temperature",
    ]
    
    # If it contains a weather term AND a time term, it's likely a weather query
    has_weather_term = any(term in q_lower for term in weather_terms)
    has_time_term = any(term in q_lower for term in time_terms)
    
    if has_weather_term and has_time_term:
        return True
        
    # Check for common weather question formats
    for pattern in weather_patterns:
        if re.search(pattern, q_lower):
            return True
            
    return False

def check_time_sensitive_patterns(q_lower):
    """
    Check if a query is time-sensitive or requires current information
    
    Args:
        q_lower: Lowercase question
        
    Returns:
        bool: True if it's time-sensitive
    """
    # Enhanced patterns for time-sensitive queries
    time_sensitive_patterns = [
        # Time indicators (high priority)
        r'\b(today|tomorrow|yesterday|tonight|this morning|this afternoon|this evening)\b',
        r'\b(current|latest|recent|now|right now|at the moment)\b',
        r'\b(this week|this month|this year|next week|next month|next year)\b',
        
        # News and events
        r'\b(news|headline|breaking|update|development|announcement)\b',
        r'\b(happened|occurring|taking place|going on|event)\b',
        
        # Sports and entertainment
        r'\b(score|game|match|playing|showing|performance|concert)\b',
        
        # Market information
        r'\b(stock price|market|rate|trading at|exchange rate)\b',
        
        # Specific dates that suggest current information
        r'\b(in 2024|in 2025|2024|2025)\b',
        
        # Questions about protests, events, or incidents
        r'\b(protest|event|incident|happening|occurred)\b',
        
        # Weather terms
        r'\b(weather|forecast|temperature|rain|snow|sunny|cloudy)\b',
    ]
    
    for pattern in time_sensitive_patterns:
        if re.search(pattern, q_lower):
            logger.info(f"Time-sensitive pattern matched: {pattern}")
            return True
            
    return False

# Keep these utility functions for fallback and debugging purposes

def is_highly_relevant(question, chunks):
    """
    Determine if the RAG chunks are highly relevant to the question
    
    Args:
        question: User question
        chunks: List of text chunks from RAG
        
    Returns:
        bool: True if chunks are highly relevant
    """
    # Extract key terms from the question
    key_terms = extract_key_terms(question)
    logger.info(f"Key terms extracted: {key_terms}")
    
    # Count how many chunks contain the key terms
    relevant_chunks = 0
    for chunk in chunks:
        if contains_key_terms(chunk, key_terms):
            relevant_chunks += 1
    
    relevance_score = relevant_chunks / len(chunks) if chunks else 0
    logger.info(f"Relevance score: {relevance_score} ({relevant_chunks}/{len(chunks)} chunks relevant)")
    
    # If more than 50% of chunks are relevant, consider it highly relevant
    return relevance_score > 0.5

def extract_key_terms(question):
    """
    Extract important terms from the question
    
    Args:
        question: User question
        
    Returns:
        list: List of key terms
    """
    q_lower = question.lower()
    
    # Remove common stop words
    stop_words = ["what", "where", "when", "how", "why", "is", "are", "the", "a", "an", "in", 
                 "on", "at", "to", "for", "with", "about", "like", "going", "be", "will", 
                 "can", "could", "would", "should", "do", "does", "did", "has", "have", "had"]
    
    for word in stop_words:
        q_lower = q_lower.replace(f" {word} ", " ")
    
    # Split into words and filter out short words
    terms = [word for word in q_lower.split() if len(word) > 3]
    
    # Remove duplicates while preserving order
    unique_terms = []
    for term in terms:
        if term not in unique_terms:
            unique_terms.append(term)
    
    return unique_terms

def contains_key_terms(chunk, key_terms):
    """
    Check if a chunk contains enough key terms to be considered relevant
    
    Args:
        chunk: Text chunk from RAG
        key_terms: List of key terms from the question
        
    Returns:
        bool: True if chunk contains enough key terms
    """
    chunk_lower = chunk.lower()
    matches = 0
    
    for term in key_terms:
        if term in chunk_lower:
            matches += 1
    
    # Require at least 2 key terms or 30% of the terms, whichever is less
    min_required = min(2, max(1, int(len(key_terms) * 0.3)))
    
    return matches >= min_required
