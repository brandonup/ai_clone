"""
Router module for deciding how to answer user queries
"""
import re
import logging
import os
from langchain_community.chat_models import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configure the confidence threshold for RAG
RAG_CONFIDENCE_THRESHOLD = 0.7  # Adjust this value based on testing

def choose_route(question, ragie_chunks):
    """
    Determine the appropriate route for answering a question
    using RAG confidence threshold and LLM-based routing
    
    Args:
        question: User question
        ragie_chunks: List of chunks retrieved from Ragie
        
    Returns:
        str: Route choice - "ragie", "force_ragie", "web", or "base"
    """
    # Normalize question for analysis
    q_lower = question.lower().strip()
    
    # First, check if RAG chunks exist and meet the confidence threshold
    if ragie_chunks:
        rag_confidence = calculate_rag_confidence(question, ragie_chunks)
        logger.info(f"RAG confidence score: {rag_confidence}")
        
        if rag_confidence >= RAG_CONFIDENCE_THRESHOLD:
            logger.info(f"Routing to 'ragie' because chunks meet confidence threshold ({rag_confidence:.2f} >= {RAG_CONFIDENCE_THRESHOLD})")
            return "ragie"
        else:
            logger.info(f"RAG chunks found but confidence too low ({rag_confidence:.2f} < {RAG_CONFIDENCE_THRESHOLD})")
            # Continue to other routing methods
    else:
        logger.info("No RAG chunks found")
    
    # If RAG confidence is low or no chunks found, check if it's a web search query
    if is_web_search_query(question):
        logger.info("Routing to 'web' based on LLM decision")
        return "web"
    
    # For non-web-search queries with RAG chunks (but low confidence), still use RAG
    if ragie_chunks:
        logger.info("Routing to 'ragie' because chunks were found (despite low confidence)")
        return "ragie"
    
    # Check for base LLM indicators (questions that don't need external knowledge)
    base_llm_patterns = [
        # General advice or opinion questions
        r'^(what (do|would) you|how (do|would|can) you)',
        r'^(can you|could you) (help|advise|suggest)',
        
        # Hypothetical scenarios
        r'(if|imagine|suppose|what if)',
        
        # Personal coaching questions
        r'(how (can|should) i|what (can|should) i)',
        r'(advice|suggestion|tip|help) (for|with|on)',
        
        # Simple factual questions that a general LLM should know
        r'^(what is|who is|how does) (a|an|the)',
    ]
    
    # Check if any base LLM patterns match
    for pattern in base_llm_patterns:
        if re.search(pattern, q_lower):
            logger.info(f"Routing to 'base' because pattern matched: {pattern}")
            return "base"
    
    # Default to trying RAG again with a modified query
    # This implements the advanced RAG optimization approach
    logger.info(f"Routing to 'force_ragie' as default")
    return "force_ragie"

def calculate_rag_confidence(question, chunks):
    """
    Calculate a confidence score for RAG chunks
    
    Args:
        question: User question
        chunks: List of text chunks from RAG
        
    Returns:
        float: Confidence score between 0 and 1
    """
    # Use the existing relevance scoring function
    # Extract key terms from the question
    key_terms = extract_key_terms(question)
    logger.info(f"Key terms extracted: {key_terms}")
    
    if not key_terms or not chunks:
        return 0.0
    
    # Calculate term frequency across all chunks
    term_matches = {}
    for term in key_terms:
        term_matches[term] = 0
        
    # Count how many chunks contain each key term
    for chunk in chunks:
        chunk_lower = chunk.lower()
        for term in key_terms:
            if term in chunk_lower:
                term_matches[term] += 1
    
    # Calculate the average term frequency across chunks
    total_matches = sum(term_matches.values())
    max_possible_matches = len(key_terms) * len(chunks)
    
    # Calculate term coverage (what percentage of terms appear at least once)
    terms_covered = sum(1 for term, count in term_matches.items() if count > 0)
    term_coverage = terms_covered / len(key_terms) if key_terms else 0
    
    # Calculate chunk relevance (what percentage of chunks contain at least some key terms)
    relevant_chunks = 0
    for chunk in chunks:
        if contains_key_terms(chunk, key_terms):
            relevant_chunks += 1
    chunk_relevance = relevant_chunks / len(chunks) if chunks else 0
    
    # Combine metrics into a single confidence score
    # Weight the metrics based on importance
    term_frequency_weight = 0.3
    term_coverage_weight = 0.4
    chunk_relevance_weight = 0.3
    
    term_frequency_score = total_matches / max_possible_matches if max_possible_matches > 0 else 0
    
    confidence_score = (
        term_frequency_score * term_frequency_weight +
        term_coverage * term_coverage_weight +
        chunk_relevance * chunk_relevance_weight
    )
    
    logger.info(f"RAG confidence calculation:")
    logger.info(f"  - Term frequency: {term_frequency_score:.2f} (weight: {term_frequency_weight})")
    logger.info(f"  - Term coverage: {term_coverage:.2f} (weight: {term_coverage_weight})")
    logger.info(f"  - Chunk relevance: {chunk_relevance:.2f} (weight: {chunk_relevance_weight})")
    logger.info(f"  = Final confidence: {confidence_score:.2f}")
    
    return confidence_score

def is_web_search_query(question):
    """
    Use the LLM to determine if a query requires web search
    
    Args:
        question: User question
        
    Returns:
        bool: True if the query should use web search
    """
    # Check if OpenAI API key is available
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        logger.error("OPENAI_API_KEY environment variable not set")
        # Fall back to pattern matching
        return check_time_sensitive_patterns(question.lower())
    
    # Create the chat model (using a smaller/cheaper model is fine for this task)
    try:
        model = ChatOpenAI(
            temperature=0.1,  # Low temperature for consistent results
            model_name="gpt-3.5-turbo",  # Could use a smaller model to save costs
            openai_api_key=openai_api_key,
            max_tokens=50  # We only need a short response
        )
        
        # Create the prompt
        system_content = """
        You are a routing assistant for an AI Coach application. Your job is to determine if a user query requires web search.
        
        Respond ONLY with "Web Search" or "Not Web Search".
        
        Use "Web Search" for:
        - Questions about current events, news, or recent developments
        - Weather forecasts or current conditions
        - Sports scores or recent game results
        - Stock prices or market information
        - Questions containing time indicators like today, yesterday, tomorrow
        - Questions about what's currently happening or trending
        - Questions about protests, events, or incidents that happened recently
        
        Use "Not Web Search" for:
        - General advice or coaching questions
        - Questions about concepts, theories, or timeless information
        - Personal development or self-improvement questions
        - Questions that don't require up-to-date information
        """
        
        # Create messages
        messages = [
            SystemMessage(content=system_content),
            HumanMessage(content=f"Query: {question}")
        ]
        
        # Get the response
        logger.info(f"Sending query to LLM for routing decision: {question}")
        response = model(messages)
        result = response.content.strip()
        
        logger.info(f"LLM routing decision for '{question}': {result}")
        
        # Return True if the response contains "Web Search"
        return "web search" in result.lower()
        
    except Exception as e:
        logger.error(f"Error in LLM routing: {str(e)}")
        # Fall back to pattern matching if LLM fails
        logger.info("Falling back to pattern matching for routing decision")
        return check_time_sensitive_patterns(question.lower())

def check_time_sensitive_patterns(q_lower):
    """
    Check if a query is time-sensitive or requires current information
    
    Args:
        q_lower: Lowercase question
        
    Returns:
        bool: True if it's time-sensitive
    """
    # Enhanced patterns for time-sensitive queries
    time_sensitive_patterns = [
        # Time indicators (high priority)
        r'\b(today|tomorrow|yesterday|tonight|this morning|this afternoon|this evening)\b',
        r'\b(current|latest|recent|now|right now|at the moment)\b',
        r'\b(this week|this month|this year|next week|next month|next year)\b',
        
        # News and events
        r'\b(news|headline|breaking|update|development|announcement)\b',
        r'\b(happened|occurring|taking place|going on|event)\b',
        
        # Sports and entertainment
        r'\b(score|game|match|playing|showing|performance|concert)\b',
        
        # Market information
        r'\b(stock price|market|rate|trading at|exchange rate)\b',
        
        # Specific dates that suggest current information
        r'\b(in 2024|in 2025|2024|2025)\b',
        
        # Questions about protests, events, or incidents
        r'\b(protest|event|incident|happening|occurred)\b',
        
        # Weather terms
        r'\b(weather|forecast|temperature|rain|snow|sunny|cloudy)\b',
    ]
    
    for pattern in time_sensitive_patterns:
        if re.search(pattern, q_lower):
            logger.info(f"Time-sensitive pattern matched: {pattern}")
            return True
            
    return False

def extract_key_terms(question):
    """
    Extract important terms from the question
    
    Args:
        question: User question
        
    Returns:
        list: List of key terms
    """
    q_lower = question.lower()
    
    # Remove common stop words
    stop_words = ["what", "where", "when", "how", "why", "is", "are", "the", "a", "an", "in", 
                 "on", "at", "to", "for", "with", "about", "like", "going", "be", "will", 
                 "can", "could", "would", "should", "do", "does", "did", "has", "have", "had"]
    
    for word in stop_words:
        q_lower = q_lower.replace(f" {word} ", " ")
    
    # Split into words and filter out short words
    terms = [word for word in q_lower.split() if len(word) > 3]
    
    # Remove duplicates while preserving order
    unique_terms = []
    for term in terms:
        if term not in unique_terms:
            unique_terms.append(term)
    
    return unique_terms

def contains_key_terms(chunk, key_terms):
    """
    Check if a chunk contains enough key terms to be considered relevant
    
    Args:
        chunk: Text chunk from RAG
        key_terms: List of key terms from the question
        
    Returns:
        bool: True if chunk contains enough key terms
    """
    chunk_lower = chunk.lower()
    matches = 0
    
    for term in key_terms:
        if term in chunk_lower:
            matches += 1
    
    # Require at least 2 key terms or 30% of the terms, whichever is less
    min_required = min(2, max(1, int(len(key_terms) * 0.3)))
    
    return matches >= min_required
