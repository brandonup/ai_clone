<!DOCTYPE html>
<html>
<head>
    <title>AI Coach App Orchestration Logic</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
        }
        h1 {
            color: #333;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
        }
        .mermaid {
            margin: 30px 0;
        }
        .notes {
            background-color: #f9f9f9;
            border-left: 4px solid #4CAF50;
            padding: 15px;
            margin: 20px 0;
        }
        .key-points {
            margin-top: 30px;
        }
        .key-points h2 {
            color: #444;
        }
        .key-points ul {
            padding-left: 20px;
        }
        .key-points li {
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>AI Coach App Orchestration Logic</h1>
        
        <div class="mermaid">
            flowchart TD
                A[User asks question] --> B[app.py: /ask route]
                B --> C[Retrieve chunks from Ragie.ai]
                
                C --> D{Router decision}
                
                %% Self-intro question detection
                D --> S1{Self-intro check}
                S1 --> |Check regex patterns| S2{Match?}
                S2 --> |Yes| F[Use base LLM]
                S2 --> |No| T1
                
                %% Time-sensitive query detection
                T1{Time-sensitive?} --> |Send to LLM| T2[LLM classification]
                T2 --> |"Web Search"| G[Use web search]
                T2 --> |"Not Web Search"| R1
                T1 --> |LLM fails| T3[Pattern fallback]
                T3 --> |Match| G
                T3 --> |No match| R1
                
                %% RAG confidence check
                R1{RAG confidence â‰¥ 0.15?} --> |Yes| E[Use RAG]
                R1 --> |No, but has chunks| H[Try modified query]
                R1 --> |No chunks| F
                
                E --> I[Generate answer with RAG context]
                F --> J[Generate answer with no context]
                
                G --> K[Perform web search]
                K --> L[Generate answer with web context]
                
                H --> M[Retrieve with modified query]
                M -->|Chunks found| N[Generate answer with new chunks]
                M -->|No chunks| O[Fall back to base LLM]
                
                I --> P[Return answer to user]
                J --> P
                L --> P
                N --> P
                O --> P
                
                %% Error handling
                I -.->|Token limit error| Q[Progressive context reduction]
                Q --> P
                
                %% Notes for key processes
                S1 -.-> S3[Patterns: "about yourself", "who are you", etc.]
                T2 -.-> T4[LLM prompt asks if query needs current info]
                R1 -.-> R2[Score based on term frequency, coverage, relevance]
                
                %% Styling
                classDef process fill:#f9f,stroke:#333,stroke-width:1px;
                classDef decision fill:#bbf,stroke:#333,stroke-width:1px;
                classDef output fill:#bfb,stroke:#333,stroke-width:1px;
                
                class A,B,C,E,F,G,H,I,J,K,L,M,N,O,Q,T2,T3 process;
                class D,S1,S2,T1,R1 decision;
                class P output;
        </div>
        
        <div class="notes">
            <h2>Key Decision Points</h2>
            <ol>
                <li><strong>Self-Introduction Detection</strong>: 
                    <ul>
                        <li>Uses regex patterns to identify questions about the coach's identity</li>
                        <li>Patterns include "about yourself", "who are you", "introduce yourself", etc.</li>
                    </ul>
                </li>
                <li><strong>Time-Sensitive Query Detection</strong>:
                    <ul>
                        <li>Sends a prompt to the LLM asking if the query requires current information</li>
                        <li>LLM analyzes for time references, current events needs, etc.</li>
                        <li>Falls back to pattern matching if LLM fails</li>
                    </ul>
                </li>
                <li><strong>RAG Confidence Evaluation</strong>:
                    <ul>
                        <li>Calculates confidence score (threshold: 0.15)</li>
                        <li>Based on term frequency, coverage, and chunk relevance</li>
                        <li>Routes to appropriate answer generation method</li>
                    </ul>
                </li>
                <li><strong>Context Management</strong>:
                    <ul>
                        <li>Implements progressive context reduction if token limits are exceeded</li>
                        <li>Preserves most relevant information when truncating</li>
                    </ul>
                </li>
            </ol>
        </div>
    </div>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default',
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            }
        });
    </script>
</body>
</html>
